# Efficient Memory Management for Large Language Model Serving with PagedAttention
## 摘要
为了提供LLM的高吞吐量服务，每次需要批量处理足够多的请求。然而现有系统面临**KV缓存内存不足**的挑战：每个请求的KV缓存内存占用巨大，且动态增减。当内存管理效率低下时，碎片化和冗余复制会造成显著的内存浪费，从而限制批处理规模。为解决这一问题，我们提出PagedAttention，这是一种受经典操作系统虚拟内存与分页技术启发的注意力算法。基于此，我们构建了vLLM这一LLM服务系统，其实现了：(1) KV缓存内存接近零浪费；(2) 支持请求内及跨请求的KV缓存灵活共享，进一步降低内存占用。评估表明，在相同延迟水平下，vLLM将主流LLM的吞吐量较FasterTransformer、Orca等最先进系统提升了2-4倍。当处理更长序列、更大模型及更复杂解码算法时，性能提升尤为显著。

## Introduction
当前LLM serving system主要面临：KV缓存的管理问题。通常做法是将一条request的KV cache存储在连续的内存空间内（大部分deep learning框架要求tensors在连续内存空间中存储）。KV cache和tensors的区别在于：
1. **KV Cache随模型生成新token，而动态增减**；
2. 其生命周期和长度无法提前预测。

因此，在两个方向上导致了内存使用的低效：
1. **内存的内外碎片**：为满足连续空间存储的要求，需要**预先分配一段连续的最大内存空间**（例如：2048 tokens），这会导致内部碎片（request的实际长度小于最大长度）；
    > 即使长度预知，预先分配也是低效的：在request的生命周期内，内存块为其保留；导致其他更短的request也无法使用当前空闲的内存块。
    
    另外，对于每个request，预分配不同长度的空间，会导致外部碎片。

2. **未实现内存共享的最大优化**：LLM通常采用advanced decoding算法（并行采样或束搜索），这些算法为每个request生成多个输出序列，可以部分共享KV Cache，但已有系统未考虑这一点。

PagedAttention的想法是什么呢？提出了页式虚拟内存机制。

将request的KV cache拆分为多个blocks：每个block包括固定数量tokens的attention keys和values。因此，KV Cache无需存储在连续内存空间。
> 联动OS的理念：将blocks当作页面；tokens当作字节；requests当作进程。
> 
> 这个设计通过使用更小尺寸的的blocks和按需分配，消除内部碎片；通过固定大小的blocks消除外部碎片。

## BackGrounds
### 基于Transformer的LLM
LLM的任务是：对token序列$(x_1, x_2, ..., x_n)$的概率模型建模。采用**自回归分解**：将整个序列的联合概率，分解为条件概率的乘积：
$$
P(x)=P(x_1)\cdot P(x_2|x_1)\cdot\cdot\cdot P(x_n|x_1,...,x_{n-1}).
$$

Transformer模型是大规模概率建模的事实标准架构，其核心组件是**自注意力层（self-attention layer）**。处理输入隐藏状态序列$(x_1, ..., x_n)\in\mathbb{R}^{n\times d}$时，首先对每个位置$i$进行线性变换生成查询（query）、键（key）和值（value）向量：
$$
q_i=W_q x_i, k_i=W_k x_i, v_i=W_v x_i.
$$
随后，该层通过计算当前位置query向量与所有历史位置的key向量的点积，得到注意力分数$a_{ij}$：
$$
a_{ij}=\frac{\exp(\frac{q_i^{T}k_j}{\sqrt{d}})}{\sum_{t=1}^{i}\exp(\frac{q_i^{T}k_t}{\sqrt{d}})}, o_i=\sum_{j=1}^{i}a_{ij}v_j.
$$

### LLM服务&自回归生成
经过训练后，大型语言模型（LLM）通常被部署为条件生成服务（例如自动补全API或聊天机器人）。向LLM服务发出的请求会提供一组input prompt tokens$(x_1,x_2,...,x_n)$，LLM服务则根据自回归分解公式，生成output tokens$(x_{n+1},x_{n+2},...,x_{n+T})$。将提示词元与输出词元的组合称为序列（sequence）。

由于自回归分解公式的分解特性，LLM只能**逐个采样生成新token**，且每个新token的生成过程都依赖于该序列中所有先前的tokens——特别是它们的键（key）和值（value）向量。在这一顺序生成过程中，现有token的键值向量，通常会被缓存以供后续token生成使用，即KV缓存（KV cache）。需要注意的是，**某个词元的KV缓存取决于其之前的所有token**，这意味着同一token出现在序列不同位置时，其KV缓存也会不同。

对于给定的一个request prompt，生成过程分为两个阶段：
1. prompt phase：以完整用户prompt$(x_1,x_2,...,x_n)$为输入，**计算首个新token的概率$P(x_{n+1}|x_1,...,x_n)$**。在此过程中，同时生成键向量$k_1,...,k_n$和值向量$v_1,...,v_n$。由于token$x_1,...,x_n$均为已知，该阶段可通过矩阵-矩阵乘法实现并行计算，因此能充分利用GPU的并行计算优势。
   
2. autoregressive generation phase：按顺序生成剩余新tokens。在第$t$次迭代时，模型接收单个token$x_{n+t}$作为输入，基于缓存的键向量$k_1,...,k_{n+t-1}$和值向量$v_1,...,v_{n+t-1}$，计算概率$P(x_{n+t+1}|x_1,...,x_{n+t})$，并生成新的键值向量$k_{n+t}$和$v_{n+t}$。该阶段在序列达到最大长度（用户指定或模型限制）或生成结束符<eos>时终止。**由于数据依赖性，不同迭代的计算无法并行化，且多采用效率较低的矩阵-向量乘法运算**，导致GPU计算资源利用率严重不足，形成内存瓶颈——这构成了单个请求延迟的主要来源。

### LLM批处理
由于同一批次内的请求共享模型权重，权重加载的开销可被批量请求均摊——当批次规模足够大时，计算开销将完全覆盖权重传输成本。然而LLM 批处理面临两大挑战：
1. **请求的异步到达**特性。若采用简单批处理策略，要么让先到请求等待后续请求（导致排队延迟），要么推迟新请求直至当前批次完成（造成吞吐量下降）。
2. **请求的输入输出长度差异巨大**。若强行通过填充（padding）对齐序列长度，将导致GPU计算资源和内存的严重浪费。

> 为解决这些问题，学界提出了**细粒度批处理机制**（如蜂窝批处理和迭代级调度）。与传统请求级批处理不同，这些技术基于迭代维度运作：每完成一次迭代，系统便移除已处理完成的请求，并动态加入新请求。这使得**新请求仅需等待单个迭代周期即可被处理**，无需阻塞至整批请求完成。此外，借助专用GPU内核，这些技术彻底消除了序列填充需求。通过降低排队延迟与填充损耗，细粒度批处理机制能显著提升LLM服务的吞吐效率。

## Methods
![alt text](image.png)

### PagedAttention
将每个序列的KV Cache分为若干个KV blocks，每个block包含：固定数量的键向量和值向量。将key block表示为：$K_j=(k_{(j-1)B+1},...,v_{jB})$；value block表示为：$V_j=(v_{(j-1)B+1},...,v_{jB})$.注意力计算公式变为：
$$
A_{ij}=\frac{\exp(\frac{q_{i}^{T}K_j}{\sqrt{d}})}{\sum_{t=1}^{\lceil \frac{i}{B}\rceil}\exp(\frac{q_{i}^{T}K_t}{\sqrt{d}})}, o_i=\sum_{j=1}^{\lceil \frac{i}{B}\rceil}V_jA_{ij}^T
$$
其中，$A_{ij}=(a_{i,(j-1)B+1},...,a_{i,jB})$是第$j$个KV block的注意力分数。

在注意力计算过程中，PagedAttention内核会动态识别、并分别获取不同的KV块。如图所示，键值向量分散存储在三个非连续物理内存块中（例如块0存储"Four score and seven"的键值向量）。
![](image-1.png)
内核执行分阶段计算：
1. **query-key交互**：每一次计算中，内核将query token（"forth"）的query向量$q_i$，与一个block内的key向量$K_j$相乘，以计算注意力分数$A_{ij}$。
2. **value聚合**：将$A_{ij}$与当前块的$V_j$相乘，生成局部注意力输出$o_i$。

总结来说，PagedAttention算法允许KV blocks存储在非连续的物理内存空间，使得vLLM中能够采用更灵活的页内存管理。

### KV Cache Manager
vLLM内存管理器的核心设计思想源于：操作系统的**虚拟内存**机制。操作系统将内存划分为固定大小的页（page），并将用户程序的逻辑页映射到物理页上——**连续的逻辑页可对应非连续的物理内存页，使得用户程序能以连续视角访问内存**。更重要的是，物理内存空间无需预先全量分配，操作系统可**按需动态分配物理页**。

vLLM将虚拟内存的思想应用于LLM服务的KV缓存管理：
1. 存储结构：
   * 通过PagedAttention将KV缓存组织为固定大小的**KV块**（类比虚拟内存中的页）；
   * 每个请求的KV缓存表示为从左到右填充的**逻辑KV块序列**，末块预留空位供未来生成使用。
2. 硬件资源管理：
   * GPU工作节点：块引擎（block engine）分配连续GPU显存，并划分为物理KV块；
   * CPU内存：同样分块以支持交换机制
3. 映射系统：
   * 块表（block table）：维护逻辑KV块与物理KV块的映射关系
   * 每个块表条目记录：
     * 逻辑块对应的物理块地址
     * 已填充位置数量

### 使用PagedAttention和vLLM解码
通过以下示例，说明vLLM如何在单输入序列的解码过程中执行PagedAttention并管理内存：
![](image-2.png)
1. prefill：与操作系统虚拟内存类似，vLLM无需预先为最大可能序列长度保留内存，而是仅分配prompt计算所需的KV块。
    * 7个prompt tokens被分配到2个逻辑KV块（块0和块1）；
    * 逻辑块映射到物理块7和1；
    * 使用常规自注意力算法，生成prompt的KV Cache和首个输出token；
    * 前4个tokens存入逻辑块0，后3个tokens存入逻辑块1（末位预留空位）。
2. 首次自回归解码
    * 基于物理块7和1执行PagedAttention生成新token；
    * 新生成的KV缓存存入逻辑块1预留槽；
    * 块表中#filled字段更新。
3. 二次解码
    * 当逻辑块1写满时，分配新逻辑块；
    * 从空闲池获取物理块3并建立映射；
    * 更新块表记录新增的逻辑-物理块对应关系。

全局来看，vLLM在每次解码迭代时执行以下关键操作：
1. 动态批处理构建：选择候选序列集合进行批处理；为新需求的逻辑KV块分配物理块。
2. 输入序列整合：将当前迭代内，所有输入tokens拼接为单一序列：提示阶段请求的所有tokens+生成阶段请求的最新token
3. 分页注意力执行：通过PagedAttention内核：访问以逻辑KV块形式存储的历史KV缓存；将新生成的KV缓存写入分配的物理KV块。

vLLM采用**动态物理块分配**机制：随着新token及其KV缓存的生成，系统持续为逻辑块分配新的物理块。其内存高效性体现在两个关键设计：
1. 紧凑的内存布局：
    * 严格遵循从左到右的填充顺序；
    * 仅当所有现存块写满时，才分配新物理块；
    * 将内存浪费严格限制在单个块容量内。
2. 弹性资源共享：
    * 请求完成生成后，立即释放其KV块，供其他请求复用；
    > ![](image-3.png)
    > 如图所示：两个序列的逻辑块，可映射到不同的物理块，实现GPU节点的内存共享。

### vLLM在其他解码场景的应用
#### 并行采样（Parallel Sampling）
对于一个输入prompt，LLM生成多个输出采样。用户可从多个候选者中，选出最喜欢的输出。

并行采样场景中，**单个请求包含：共享相同输入prompt的多个输出样本**，这使得prompt的KV缓存也可被共享。借助PagedAttention和分页内存管理机制，vLLM能够轻松实现这种内存共享优化。共享机制的实现如下图：
![](image-4.png)

1. prompt阶段：双输出样本共享相同的prompt，因此只保留一份prompt状态的拷贝；**两个序列的prompts对应逻辑块，映射至相同的物理块**。
   * 逻辑块映射：序列A1/A2的逻辑块0 → 物理块7；序列A1/A2的逻辑块1 → 物理块1
   * 物理块引用计数：物理块7和1的引用计数均为2
2. generation阶段：**写时复制机制（copy-on-write）**
   * 当样本A1需修改逻辑块1时：检测物理块1引用计数>1；分配新物理块3并复制原数据；物理块1引用计数降为1
   * 样本A2写入物理块1时：引用计数已为1，直接写入

vLLM的技术优势：
* 内存节省：多个输出共享prompt的KV缓存，显著减少长提示词场景的内存占用；
* 安全隔离：块级写时复制，确保多样本修改隔离性；
* 零冗余设计：仅末位逻辑块需写时复制，其余物理块完全共享。

#### 束搜索（Beam Search）

在机器翻译等LLM任务中，束搜索用于获取**最优k个输出**。通过束宽参数$k$，控制每一步保留的候选序列数，有效避免全量遍历样本空间的计算复杂度。其工作流程分为三步：

1. 候选扩展：对束内的每个候选序列，枚举词汇表$V$的所有可能续接tokens；
2. 概率评估：调用LLM计算$k\times |V|$个候选序列各自的生成概率（$|V|$为词汇表大小）
3. 择优保留：筛选概率最高的$k$个序列，进入下一轮迭代。

与并行解码不同，束搜索实现了更深层次的KV块共享机制：不止共享prompt对应block，**不同候选序列也共享对应blocks，共享机制随着解码过程动态迭代**。

1. 动态共享拓扑：
   * 所有候选序列，强制共享首个block（prompt block 0）
   * 候选序列3从第2块开始分叉；候选序列0-2共享前3块，在第四块分叉
   * 淘汰候选序列（0和3）时自动释放其逻辑块
2. 智能内存管理：
   * 引用计数归零的物理块即时释放；
   * 为新候选序列动态分配物理块（块9-12）

![image-20250410172841152](/Users/lisa/Library/Application Support/typora-user-images/image-20250410172841152.png)

#### 共享前缀

在LLM应用中，用户通常需要提供**包含instructions和example inputs/outputs**的**系统提示词（system prompt）**，这些内容会与实际任务input拼接，形成完整prompt。此类共享prefix可通过提示词工程进一步微调，以提升下游任务的准确率。vLLM的实现方式如下：

1. **预缓存机制**：预先将共享prefix的KV缓存，存入专用物理块（类比OS对共享库的内存管理）；
2. 动态映射：含有共享prefix的用户请求，可直接将逻辑块映射到已缓存的物理块（末位块标记为copy-on-write）；
3. 计算优化：prompt phase仅需执行用户独有输入的计算（消除对共享prefix的冗余计算）



### 调度与抢占机制

当请求流量超过系统容量时，vLLM优先处理部分请求。vLLM采用**先来先服务（FCFS）**算法，以确保公平性并避免请求饥饿。

LLM服务面临的挑战有：输入prompts的长度差异显著；输出长度无法预知（由输入和模型行为决定）。随着请求数量和输出数量增加，VLLM可能会耗尽GPU的物理块，以致无法存储新生成的KV Cache。对此有两个亟需解决的问题：

1. **块驱逐**策略：通常使用启发式算法，预测最晚访问的物理块

   * **全有或全无（All-or-Nothing）原则**：同一序列的所有blocks，必须同时被驱逐或保留（由于一个序列的所有blocks同时被访问）；
   * **组调度（Gang-Scheduling）**：同一请求内的多序列（如束搜索中的候选序列）作为**序列组**统一调度（需要避免破坏序列间潜在的内存共享关系）

2. **驱逐块恢复**：

   * **内存交换（Swapping）**：将被驱逐的KV块，暂存至CPU RAM。工作流程如下：

     * 一旦GPU中没有空闲的物理块以分配给新token，选择待驱逐的序列组；
     * 将该序列组的所有KV块，整体迁移至CPU RAM（在此期间，vLLM暂停接收新需求，直至所有被抢占的序列迁移完成）；
     * 一旦请求完成，从GPU内存释放其所有blocks，被抢占的序列从CPU中迁移回GPU，继续计算。

     > 注意：CPU RAM永不超过GPU RAM中的物理块总数，因此：CPU RAM中交换空间大小严格受限于GPU显存容量。

   * **重计算（Recomputation）**：当被抢占的序列被重新调度时，重新计算其KV Cache。

     * 加速机制：将被抢占序列的已解码tokens，与原始用户prmpt，拼接形成新prompt；通过单次prompt阶段（prefill phase）并行，重构完整KV缓存。



### 分布式执行

许多LLMs的参数量超过了单个GPU的容量。因此，需要将参数分区并分布到多个GPU上，并采用模型并行策略处理。vLLM通过以下机制实现分布式部署：

1. **模型并行架构**：**Megatron-LM**风格的**张量并行**策略

   基于**SPMD**的执行模式：

   * **线性层**：块状矩阵乘法分区计算
   * **注意力层**：按注意力头维度切分（每个SPMD进程处理一部分注意力头）
   * **同步机制**：通过all-reduce操作同步中间结果

2. **全局KV缓存管理**：（每个GPU处理相同的输入tokens）

   * 采用**集中式调度器**统一管理：维护逻辑块到物理块的全局映射（所有GPU共享）；为每个请求，分配物理块ID
   * 分布式存储：相同物理块ID在不同GPU存储不同内容（对应各自分片的注意力头KV Cache）；各GPU仅保留自身注意力头对应的KV Cache分片

#### 工作流程

1. **调度器预处理阶段**：
   * 对于batch中的每个请求，生成**包含输入tokens的ID的集合**，和**逻辑-物理块映射表（Block Table）**；
   * 将控制信息（token IDs+Block Table）**广播至所有GPU workers**；
2. **GPU workers并行计算阶段**：
   * 注意力层：根据控制信息中的块表，读取对应的KV Cache；各worker独立处理分配的注意力头子集；
   * 全局同步：通过all-reduce原语自动同步中间结果（无需调度器介入）
3. **回收迭代结果**：GPU workers将采样生成的tokens回传至调度器。

vLLM仅需在每个解码迭代开始时，一次性同步由调度器下发的控制信息包含的内存状态；执行期间无需额外同步内存状态。



## Implementation

vLLM作为端到端的LLM服务系统，采用分层架构设计：

1. **前端接口层**：基于FastAPI构建RESTful服务，完整支持OpenAI API协议；其可定制的参数包括：最大序列长度，束搜索宽度$k$，温度系数等采样参数；
2. **核心引擎层**：**控制平台**（8.5K Python代码）包括分布式调度器和块管理器；**数据平台**（2K C++/CUDA代码）包括PagedAttention定制内核和高并发内存操作原语。集成PyTorch与HuggingFace Transformers等，原生适配：GPT系列，OPT和LLaMA等主流架构。
3. **分布式通信层**：基于NCCL实现跨GPU张量高效同步，和全兼容Megatron-LM的并行模式。



### 内核级优化

针对PagedAttention的特有内存访问模式，vLLM开发了三大定制化GPU内核：

1. **融合式KV缓存写入**：在每个Transformer层，KV Cache被划分为若干个blocks，重构为一个为读取blocks而优化的内存布局，再按块表写入。
   * 传统方案需多次内核启动完成；而当前将三级操作融合为单一内核。
2. **块感知注意力计算**：基于FasterTransformer内核改造，使得每个GPU warp专门负责读取单个KV块，支持动态批处理（变长序列混合计算）。
   * 该方法强制合并内存访问，实现块内计算零拷贝。
3. **批量块拷贝**：传统的`cudaMemcpyAsync`导致碎片化小拷贝；因此该方法实现非连续块拷贝操作批量提交，采用写时复制。



### 解码算法支持框架

vLLM通过三大原子操作实现多样化解码算法：

| 操作   | 功能描述               | 典型应用场景                |
| ------ | ---------------------- | --------------------------- |
| fork   | 从现有序列克隆新序列   | 并行采样/束搜索候选分支     |
| append | 追加新tokens到指定序列 | 自回归生成迭代step          |
| free   | 释放序列及其KV Cache   | 终止条件触发/低概率路径修剪 |
